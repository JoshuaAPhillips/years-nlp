{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 22:30:32,805 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "/Users/Joshua/Documents/years-nlp/venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "2024-07-01 22:30:33,127 - top2vec - INFO - Downloading universal-sentence-encoder-multilingual model\n",
      "INFO:top2vec:Downloading universal-sentence-encoder-multilingual model\n",
      "2024-07-01 22:30:34,354 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2024-07-01 22:31:14,301 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "/Users/Joshua/Documents/years-nlp/venv/lib/python3.11/site-packages/umap/umap_.py:2437: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "2024-07-01 22:31:17,274 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2024-07-01 22:31:17,282 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m documents \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTop2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeep-learn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop2vec_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/years-nlp/venv/lib/python3.11/site-packages/top2vec/Top2Vec.py:705\u001b[0m, in \u001b[0;36mTop2Vec.__init__\u001b[0;34m(self, documents, min_count, topic_merge_delta, ngram_vocab, ngram_vocab_args, embedding_model, embedding_model_path, embedding_batch_size, split_documents, document_chunker, chunk_length, max_num_chunks, chunk_overlap_ratio, chunk_len_coverage_ratio, sentencizer, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, umap_args, gpu_umap, hdbscan_args, gpu_hdbscan, index_topics, verbose)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserialized_topic_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopics_indexed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mumap_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mumap_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mhdbscan_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhdbscan_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtopic_merge_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopic_merge_delta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mgpu_umap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpu_umap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mgpu_hdbscan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpu_hdbscan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mindex_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# initialize document indexing variables\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/years-nlp/venv/lib/python3.11/site-packages/top2vec/Top2Vec.py:1394\u001b[0m, in \u001b[0;36mTop2Vec.compute_topics\u001b[0;34m(self, umap_args, hdbscan_args, topic_merge_delta, gpu_umap, gpu_hdbscan, index_topics)\u001b[0m\n\u001b[1;32m   1391\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinding topics\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;66;03m# create topic vectors\u001b[39;00m\n\u001b[0;32m-> 1394\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_topic_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;66;03m# deduplicate topics\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deduplicate_topics(topic_merge_delta)\n",
      "File \u001b[0;32m~/Documents/years-nlp/venv/lib/python3.11/site-packages/top2vec/Top2Vec.py:891\u001b[0m, in \u001b[0;36mTop2Vec._create_topic_vectors\u001b[0;34m(self, cluster_labels)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m unique_labels:\n\u001b[1;32m    889\u001b[0m     unique_labels\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopic_vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_l2_normalize(\n\u001b[0;32m--> 891\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_vectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43munique_labels\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/years-nlp/venv/lib/python3.11/site-packages/numpy/core/shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from top2vec import Top2Vec\n",
    "import pandas as pd\n",
    "import natsort\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# documents\n",
    "df = pd.DataFrame(columns=[\"idno\", \"text\"])\n",
    "\n",
    "corpus_paths = glob.glob(\"data/*.txt\")\n",
    "corpus_paths = natsort.natsorted(corpus_paths)\n",
    "\n",
    "for path in corpus_paths:\n",
    "  with open(path, 'r') as file:\n",
    "    idd = os.path.basename(path).split('.')[0]\n",
    "    text = file.read().replace('\\n', ' ')\n",
    "    df = pd.concat([df, pd.DataFrame([[idd, text]], columns=[\"idno\", \"text\"])])\n",
    "\n",
    "documents = df[\"text\"].tolist()\n",
    "\n",
    "# train the model\n",
    "model = Top2Vec(documents, min_count=2, speed=\"deep-learn\", workers=4)\n",
    "model.save(\"top2vec_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
